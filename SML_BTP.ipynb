{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5WRyYpadiAS"
      },
      "source": [
        "# **SML Project: Binary Tree Predictors**\n",
        "\n",
        "*   **Author:** Matteo Onger\n",
        "*   **Date:** October 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2eLQPrrgfbd"
      },
      "source": [
        "**Dataset documentation**:\n",
        "*   [Secondary Mushroom](https://archive.ics.uci.edu/dataset/848/secondary+mushroom+dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh6pPVhJLCTm"
      },
      "source": [
        "## VM Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S95Nvs8sdsQ-",
        "outputId": "e2140ef6-6a11-47ef-d9c4-02a257d78410"
      },
      "outputs": [],
      "source": [
        "# install dataset package\n",
        "!pip install ucimlrepo\n",
        "\n",
        "# download repository\n",
        "!git clone -b dev https://github.com/MatteoOnger/SML_Project.git\n",
        "\n",
        "# set working directory\n",
        "%cd /content/SML_Project/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93gz7BMvLLrX"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PCSp96pJdllH"
      },
      "outputs": [],
      "source": [
        "# ---- LIBRARIES ----\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from typing import Any, Dict, List, Tuple, Type\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "from binrandomforest import BinRandomForest\n",
        "from bintreepredictor import BinTreePredictor\n",
        "from data import DataSet\n",
        "from utils import round_wrp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nv8w9CYxnUP1"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LJiQJtcoUVai"
      },
      "outputs": [],
      "source": [
        "# ---- FUNCTIONS ----\n",
        "def k_folds_cross_val(\n",
        "        k :int,\n",
        "        predictor :BinRandomForest|BinTreePredictor,\n",
        "        data :pd.DataFrame,\n",
        "        label_col :str,\n",
        "        shuffle :bool=True,\n",
        "        random_state :int=1,\n",
        "        verbose :bool=False\n",
        "    ) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Applies the k-folds cross validation to estimate the expected risk of the predictor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "        Number of folds.\n",
        "    predictor : BinRandomForest | BinTreePredictor\n",
        "        Predictor that must be tested.\n",
        "    data : pd.DataFrame\n",
        "        Data used to train and test the predictor.\n",
        "    label_col : str\n",
        "        The name of the column of ``data`` that contains the labels.\n",
        "    shuffle : bool, optional\n",
        "        Whether to shuffle the data before splitting into batches, by default True.\n",
        "    random_state : int, optional\n",
        "        When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold. \n",
        "        Otherwise, this parameter has no effect, by default 1.\n",
        "    verbose : bool, optional\n",
        "        If True, training and test error of each fold are printed, by default False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        The tuple returned contains the average training error and the average test error.\n",
        "    \"\"\"\n",
        "    avg_train_err = 0\n",
        "    avg_test_err = 0\n",
        "\n",
        "    cv = KFold(n_splits=k, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(cv.split(data)):\n",
        "        train_ds = DataSet(data.iloc[train_index], label_col=label_col)\n",
        "        test_ds =  DataSet(data.iloc[test_index], label_col=label_col)\n",
        "\n",
        "        train_err = predictor.fit(train_ds)\n",
        "        _, test_err = predictor.predict(test_ds)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"round {i} - training_err:{round_wrp(train_err,4)} - test_err:{round_wrp(test_err,4)}\")\n",
        "\n",
        "        avg_train_err += train_err\n",
        "        avg_test_err += test_err\n",
        "    return avg_train_err / k, avg_test_err / k\n",
        "\n",
        "\n",
        "def nested_cross_val(\n",
        "        outer_k :int,\n",
        "        inner_k :int,\n",
        "        predicotr_class :Type[BinTreePredictor]|Type[BinRandomForest],\n",
        "        fixed_hyperparams :Dict[str, Any],\n",
        "        hyperparams :List[Dict[str, Any]],\n",
        "        data :pd.DataFrame,\n",
        "        label_col :str,\n",
        "        shuffle :bool=True,\n",
        "        random_state :int=1,\n",
        "        verbose :bool=False\n",
        "    ) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Applies the nested cross validation to estimate the expected risk of the learning algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    outer_k : int\n",
        "        Number of folds of the outer CV.\n",
        "    inner_k : int\n",
        "        Number of folds of the inner CV.\n",
        "    predicotr_class : Type[BinTreePredictor] | Type[BinRandomForest]\n",
        "        Class of the predictor that must be tested.\n",
        "    fixed_hyperparams : Dict[str, Any]\n",
        "        Fixed hyper-parameters on which no tuning is performed.\n",
        "    hyperparams : List[Dict[str, Any]]\n",
        "        Hyper-parameters to be tuned.\n",
        "        A list containing all the combinations of the hyper-parameters to try must be given, then the best combination will be used. \n",
        "    data : pd.DataFrame\n",
        "        Data used to train and test the predictor.\n",
        "    label_col : str\n",
        "        The name of the column of ``data`` that contains the labels.\n",
        "    shuffle : bool, optional\n",
        "        Whether to shuffle the data before splitting into batches, by default True.\n",
        "    random_state : int, optional\n",
        "        When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold. \n",
        "        Otherwise, this parameter has no effect, by default 1.\n",
        "    verbose : bool, optional\n",
        "        If True, training and test error of each fold are printed, by default False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        The tuple returned contains the average training error and the average test error.\n",
        "    \"\"\"\n",
        "    avg_train_err = 0\n",
        "    avg_test_err = 0\n",
        "\n",
        "    outer_cv = KFold(n_splits=outer_k, shuffle=shuffle, random_state=random_state)\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(outer_cv.split(data)):\n",
        "        train_df = data.iloc[train_index]\n",
        "        test_df = data.iloc[test_index]\n",
        "\n",
        "        train_ds = DataSet(train_df, label_col=label_col)\n",
        "        test_ds = DataSet(test_df, label_col=label_col)\n",
        "\n",
        "        best_hyperparam, best_val_err = None, float(\"inf\")\n",
        "\n",
        "        for j, hp in enumerate(hyperparams): \n",
        "            predictor = predicotr_class(**fixed_hyperparams, **hp, id=j)\n",
        "            _, avg_val_err = k_folds_cross_val(inner_k, predictor, train_df, label_col, shuffle, random_state)\n",
        "\n",
        "            if avg_val_err < best_val_err:\n",
        "                best_val_err = avg_val_err\n",
        "                best_hyperparam = hp\n",
        "\n",
        "        predictor = predicotr_class(**fixed_hyperparams, **best_hyperparam, id=-i)\n",
        "        train_err = predictor.fit(train_ds)\n",
        "        _, test_err = predictor.predict(test_ds)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"round {i} - training_err:{round_wrp(train_err,4)} - validation_err:{round_wrp(best_val_err,4)} - test_err:{round_wrp(test_err,4)}\")\n",
        "\n",
        "        avg_train_err += train_err\n",
        "        avg_test_err += test_err\n",
        "    return avg_train_err / outer_k, avg_test_err / outer_k\n",
        "\n",
        "\n",
        "def eval_hyperparams(\n",
        "        k :int,\n",
        "        predicotr_class :Type[BinTreePredictor]|Type[BinRandomForest],\n",
        "        fixed_hyperparams :Dict[str, Any],\n",
        "        hyperparams :List[Dict[str, Any]],\n",
        "        data :pd.DataFrame,\n",
        "        label_col :str,\n",
        "        shuffle :bool=True,\n",
        "        random_state :int=1,\n",
        "        verbose :bool=False\n",
        "    ) -> Dict[int, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Trains one predictor for each combination of the hyper-parameters given in ``hyperparams`` and then\n",
        "    applies the k-folds cross validation to estimate its expected risk.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "        Number of folds.\n",
        "    predicotr_class : Type[BinTreePredictor] | Type[BinRandomForest]\n",
        "        Class of the predictor that must be tested.\n",
        "    fixed_hyperparams : Dict[str, Any]\n",
        "        Fixed hyperparameters.\n",
        "    hyperparams : List[Dict[str, Any]]\n",
        "        A list containing all the combinations of the hyperparameters to try.\n",
        "    data : pd.DataFrame\n",
        "        Data used to train and test the predictor.\n",
        "    label_col : str\n",
        "        The name of the column of ``data`` that contains the labels.\n",
        "    shuffle : bool, optional\n",
        "        Whether to shuffle the data before splitting into batches, by default True.\n",
        "    random_state : int, optional\n",
        "        When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold. \n",
        "        Otherwise, this parameter has no effect, by default 1.\n",
        "    verbose : bool, optional\n",
        "        If True, training and test error of each fold are printed, by default False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[int, Dict[str, float]]\n",
        "        _description_\n",
        "    \"\"\"\n",
        "    results = dict()\n",
        "    for i, hp in enumerate(hyperparams): \n",
        "        predictor = predicotr_class(**fixed_hyperparams, **hp, id=i)\n",
        "        avg_train_err, avg_test_err = k_folds_cross_val(k, predictor, data, label_col, shuffle, random_state, verbose)\n",
        "        results[i] = {\"avg_train_err\":avg_train_err, \"avg_test_err\":avg_test_err}\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c5_g4VGfAI-W"
      },
      "outputs": [],
      "source": [
        "# fetch datatset\n",
        "mushroom_df = fetch_ucirepo(id=848).data.original\n",
        "mushroom_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Binary Tree Predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nsv9LNoWTZC"
      },
      "outputs": [],
      "source": [
        "# use k-folds CV to estimate the expected risk of the predictors produced by setting to different values the hyper-parameters\n",
        "k = 5\n",
        "\n",
        "fixed_hyperparams = {\n",
        "    \"loss_func\":\"zero-one\", \n",
        "    \"prediction_criterion\":\"mode\",\n",
        "    \"stop_criterion\":\"max_nodes\",\n",
        "    \"max_features\":None,\n",
        "    \"max_thresholds\":5,\n",
        "}\n",
        "\n",
        "hyperparams = [\n",
        "    {\"split_criterion\":\"entropy\", \"stop_criterion_threshold\": 96},\n",
        "    {\"split_criterion\":\"entropy\", \"stop_criterion_threshold\":128},\n",
        "    {\"split_criterion\":\"entropy\", \"stop_criterion_threshold\":160},\n",
        "    {\"split_criterion\":\"gini\", \"stop_criterion_threshold\": 96},\n",
        "    {\"split_criterion\":\"gini\", \"stop_criterion_threshold\":128},\n",
        "    {\"split_criterion\":\"gini\", \"stop_criterion_threshold\":160},\n",
        "    {\"split_criterion\":\"misclass\", \"stop_criterion_threshold\": 96},\n",
        "    {\"split_criterion\":\"misclass\", \"stop_criterion_threshold\":128},\n",
        "    {\"split_criterion\":\"misclass\", \"stop_criterion_threshold\":160},\n",
        "]\n",
        "\n",
        "eval_hyperparams(k, BinTreePredictor, fixed_hyperparams, hyperparams, mushroom_df, \"class\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the predictor, test the performance and print the results\n",
        "train_ds = DataSet(mushroom_df.sample(frac=0.8, random_state=2106))\n",
        "test_ds = DataSet(mushroom_df.drop(train_ds.index))\n",
        "\n",
        "tree = BinTreePredictor(\"zero-one\", \"mode\", \"gini\", \"max_height\", 20, max_thresholds=5)\n",
        "train_err = tree.fit(train_ds)\n",
        "prediction, test_err = tree.predict(test_ds)\n",
        "\n",
        "tree.print_tree()\n",
        "print(f\"Training error:{train_err}\")\n",
        "print(f\"Test error:{test_err}\")\n",
        "\n",
        "res = mushroom_df.copy(deep=True)\n",
        "res.insert(1, \"predicted-class\", prediction)\n",
        "\n",
        "print(res)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
